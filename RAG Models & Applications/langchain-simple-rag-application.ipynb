{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q torch transformers accelerate bitsandbytes transformers sentence-transformers faiss-gpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T18:27:44.243718Z","iopub.execute_input":"2025-08-12T18:27:44.243939Z","iopub.status.idle":"2025-08-12T18:27:46.625864Z","shell.execute_reply.started":"2025-08-12T18:27:44.243924Z","shell.execute_reply":"2025-08-12T18:27:46.625191Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q langchain langchain-community","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T18:27:49.361612Z","iopub.execute_input":"2025-08-12T18:27:49.362286Z","iopub.status.idle":"2025-08-12T18:27:55.197484Z","shell.execute_reply.started":"2025-08-12T18:27:49.362253Z","shell.execute_reply":"2025-08-12T18:27:55.196819Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from getpass import getpass\nACCESS_TOKEN = getpass(\"GITHUB_PERSONAL_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T18:27:59.391941Z","iopub.execute_input":"2025-08-12T18:27:59.392581Z","iopub.status.idle":"2025-08-12T18:28:14.591053Z","shell.execute_reply.started":"2025-08-12T18:27:59.392550Z","shell.execute_reply":"2025-08-12T18:28:14.590468Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.document_loaders import GitHubIssuesLoader\n\nloader = GitHubIssuesLoader(\n    repo=\"huggingface/peft\",\n    access_token=ACCESS_TOKEN,\n    include_prs=False,\n    state=\"all\"\n)\n\ndocs = loader.load()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T18:28:17.502360Z","iopub.execute_input":"2025-08-12T18:28:17.502922Z","iopub.status.idle":"2025-08-12T18:29:03.492243Z","shell.execute_reply.started":"2025-08-12T18:28:17.502896Z","shell.execute_reply":"2025-08-12T18:29:03.491436Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\n\nsplitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=30)\n\nchunked_docs = splitter.split_documents(docs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T18:29:19.925233Z","iopub.execute_input":"2025-08-12T18:29:19.925868Z","iopub.status.idle":"2025-08-12T18:29:20.227984Z","shell.execute_reply.started":"2025-08-12T18:29:19.925843Z","shell.execute_reply":"2025-08-12T18:29:20.227293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install faiss-cpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T18:32:30.170230Z","iopub.execute_input":"2025-08-12T18:32:30.170963Z","iopub.status.idle":"2025-08-12T18:32:34.942206Z","shell.execute_reply.started":"2025-08-12T18:32:30.170934Z","shell.execute_reply":"2025-08-12T18:32:34.941259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.vectorstores import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\ndb = FAISS.from_documents(chunked_docs,\n                          HuggingFaceEmbeddings(model_name='BAAI/bge-base-en-v1.5'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T18:32:46.793304Z","iopub.execute_input":"2025-08-12T18:32:46.793597Z","iopub.status.idle":"2025-08-12T18:34:39.338420Z","shell.execute_reply.started":"2025-08-12T18:32:46.793572Z","shell.execute_reply":"2025-08-12T18:34:39.337797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install faiss-cpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T17:44:17.413265Z","iopub.execute_input":"2025-08-12T17:44:17.413540Z","iopub.status.idle":"2025-08-12T17:44:20.470842Z","shell.execute_reply.started":"2025-08-12T17:44:17.413518Z","shell.execute_reply":"2025-08-12T17:44:20.469937Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"retriever = db.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs={'k': 4}\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T18:34:46.046076Z","iopub.execute_input":"2025-08-12T18:34:46.046424Z","iopub.status.idle":"2025-08-12T18:34:46.050850Z","shell.execute_reply.started":"2025-08-12T18:34:46.046400Z","shell.execute_reply":"2025-08-12T18:34:46.050235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import importlib.metadata as m, transformers, os, sys, torch\nprint(\"transformers:\", m.version(\"transformers\"))\nprint(\"file:\", transformers.__file__)\nprint(\"tokenizers:\", m.version(\"tokenizers\"))\nprint(\"accelerate:\", m.version(\"accelerate\"))\nprint(\"bitsandbytes:\", m.version(\"bitsandbytes\") if \"bitsandbytes\" in sys.modules else \"not imported\")\nprint(\"CUDA available:\", torch.cuda.is_available())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T18:46:01.467649Z","iopub.execute_input":"2025-08-12T18:46:01.467969Z","iopub.status.idle":"2025-08-12T18:46:01.478438Z","shell.execute_reply.started":"2025-08-12T18:46:01.467948Z","shell.execute_reply":"2025-08-12T18:46:01.477800Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y transformers tokenizers accelerate\n!pip install --no-cache-dir \"transformers==4.45.2\" \"tokenizers>=0.19.1\" \"accelerate>=0.33.0\" safetensors einops sentencepiece\n# if you want 4-bit\n!pip install --no-cache-dir bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T18:46:33.931668Z","iopub.execute_input":"2025-08-12T18:46:33.931955Z","iopub.status.idle":"2025-08-12T18:46:49.034762Z","shell.execute_reply.started":"2025-08-12T18:46:33.931933Z","shell.execute_reply":"2025-08-12T18:46:49.034029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y transformers tokenizers accelerate\n!pip install --no-cache-dir \"transformers==4.45.2\" \"tokenizers>=0.19.1\" \"accelerate>=0.33.0\" safetensors einops sentencepiece bitsandbytes-windows","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T18:47:11.841389Z","iopub.execute_input":"2025-08-12T18:47:11.841922Z","iopub.status.idle":"2025-08-12T18:47:23.597343Z","shell.execute_reply.started":"2025-08-12T18:47:11.841893Z","shell.execute_reply":"2025-08-12T18:47:23.596363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T18:48:52.030631Z","iopub.execute_input":"2025-08-12T18:48:52.031268Z","iopub.status.idle":"2025-08-12T18:48:55.228751Z","shell.execute_reply.started":"2025-08-12T18:48:52.031244Z","shell.execute_reply":"2025-08-12T18:48:55.228023Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T18:49:23.166259Z","iopub.execute_input":"2025-08-12T18:49:23.166490Z","iopub.status.idle":"2025-08-12T18:49:26.363928Z","shell.execute_reply.started":"2025-08-12T18:49:23.166475Z","shell.execute_reply":"2025-08-12T18:49:26.363218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nmodel_name = \"HuggingFaceH4/zephyr-7b-beta\"\n\nbnb = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\ntok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb,\n    device_map=\"auto\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T18:49:31.482658Z","iopub.execute_input":"2025-08-12T18:49:31.483311Z","iopub.status.idle":"2025-08-12T18:49:31.892249Z","shell.execute_reply.started":"2025-08-12T18:49:31.483280Z","shell.execute_reply":"2025-08-12T18:49:31.891259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.llms import HuggingFacePipeline\nfrom langchain.prompts import PromptTemplate\nfrom transformers import pipeline\nfrom langchain_core.output_parsers import StrOutputParser\n\ntext_generation_pipeline = pipeline(\n    model=model,\n    tokenizer=tokenizer,\n    task=\"text-generation\",\n    temperature=0.2,\n    do_sample=True,\n    repetition_penalty=1.1,\n    return_full_text=True,\n    max_new_tokens=400,\n)\n\nllm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n\nprompt_template = \"\"\"\n<|system|>\nAnswer the question based on your knowledge. Use the following context to help:\n\n{context}\n\n</s>\n<|user|>\n{question}\n</s>\n<|assistant|>\n\n \"\"\"\n\nprompt = PromptTemplate(\n    input_variables=[\"context\", \"question\"],\n    template=prompt_template,\n)\n\nllm_chain = prompt | llm | StrOutputParser()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain_core.runnables import RunnablePassthrough\n\nretriever = db.as_retriever()\n\nrag_chain = (\n {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | llm_chain\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}